{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ML Sklearn"
      ],
      "metadata": {
        "id": "VdaOIkqvI1gG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load RS models from Github\n",
        "url = f\"https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/src/TabRS.py\"\n",
        "!wget --no-cache --backups=1 {url}\n",
        "\n",
        "from TabRS import rmse, GlobalMeanRS, UserMeanRS, ItemMeanRS, SVD\n",
        "# rmse: turth, pred\n",
        "# UserMeanRS: n_users, min_data\n",
        "# SVD: n_users, n_items, lam, K, iterNum, tol, verbose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQBMyN0ENcXp",
        "outputId": "295274f2-08ec-4b10-9acd-0a1247a2242a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-19 15:01:00--  https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/src/TabRS.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9001 (8.8K) [text/plain]\n",
            "Failed to rename TabRS.py to TabRS.py.1: (2) No such file or directory\n",
            "Saving to: ‘TabRS.py’\n",
            "\n",
            "\rTabRS.py              0%[                    ]       0  --.-KB/s               \rTabRS.py            100%[===================>]   8.79K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-11-19 15:01:00 (19.7 MB/s) - ‘TabRS.py’ saved [9001/9001]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "# Prepare Data: Type(1)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('url_to_data')\n",
        "X, y = data.values[:,0:2], data.values[:,2] # From df to np array\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "# Prepare Data: Type(2)\n",
        "train = pd.read_csv('url_to_train')\n",
        "test = pd.read_csv('url_to_test')\n",
        "X_train, y_train = train.values[:,0:2], train.values[:,2]\n",
        "X_test = test.values\n",
        "\n",
        "# Feature Standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X[:, 0] = StandardScaler().fit_transform(X[:, 0].reshape(-1, 1)).flatten() # For a specific feature\n",
        "\n",
        "# Label Standardization\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X = np.concatenate([X_train, X_test], axis=0)\n",
        "user_le, item_le = LabelEncoder(), LabelEncoder()\n",
        "\n",
        "user_le.fit(X[:,0]) # fit the encoder\n",
        "item_le.fit(X[:,1])\n",
        "\n",
        "X_train[:,0] = user_le.transform(X_train[:,0]) # user\n",
        "X_test[:,0] = user_le.transform(X_test[:,0])\n",
        "\n",
        "X_train[:,1] = item_le.transform(X_train[:,1]) # item\n",
        "X_test[:,1] = item_le.transform(X_test[:,1])\n",
        "\n",
        "n_users = len(user_le.classes_)\n",
        "n_items = len(item_le.classes_)"
      ],
      "metadata": {
        "id": "xB2Ad7OYNBC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic method: User/Item Mean\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class UserItemAverage(BaseEstimator):\n",
        "    def __init__(self, feats, target, min_data):\n",
        "        self.feats = feats\n",
        "        self.target = target\n",
        "        self.min_data = min_data\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.glb_avg = X[self.target].mean()\n",
        "\n",
        "        user_stats = X.groupby(feats[1])[self.target].agg(['mean','count'])\n",
        "        item_stats = X.groupby(feats[0])[self.target].agg(['mean','count'])\n",
        "\n",
        "        self.user_avg = user_stats['mean'].where(user_stats['count'] >= self.min_data, self.glb_avg).to_dict()\n",
        "        self.item_avg = item_stats['mean'].where(item_stats['count'] >= self.min_data, self.glb_avg).to_dict()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        user_avg = X[feats[1]].map(self.user_avg).fillna(self.glb_avg)\n",
        "        item_avg = X[feats[0]].map(self.item_avg).fillna(self.glb_avg)\n",
        "\n",
        "        return (user_avg + item_avg)/2\n",
        "\n",
        "# For the implementation I would use dataframe instead of numpy arrays\n",
        "class UserMedianRS(BaseEstimator):\n",
        "    def fit(self, X_train):\n",
        "        self.glb_median = X_train['rating'].median()\n",
        "        self.user_median = X_train.groupby('user_id')['rating'].median()\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return X_test['user_id'].map(self.user_median).fillna(self.glb_median)"
      ],
      "metadata": {
        "id": "05BqP8J8K693"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced method: SVD\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "class SVD(BaseEstimator):\n",
        "    \"\"\"\n",
        "    Matrix Factorization (MF) class for collaborative filtering using ALS with Static Bias.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_users, n_items, lam=.01, K=10, iterNum=10, tol=1e-4, verbose=1):\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.lam = lam\n",
        "        self.K = K\n",
        "        self.iterNum = iterNum\n",
        "        self.tol = tol\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Parameters initialization\n",
        "        self.mu = 0.0\n",
        "        self.a = np.zeros(n_users)\n",
        "        self.b = np.zeros(n_items)\n",
        "        self.P = np.random.normal(scale=1./self.K, size=(n_users, K))\n",
        "        self.Q = np.random.normal(scale=1./self.K, size=(n_items, K))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fits the matrix factorization model to the given data.\n",
        "        X: (n_samples, 2) -> pairs of [user_id, item_id]\n",
        "        y: (n_samples,) -> ratings\n",
        "        \"\"\"\n",
        "        n_obs = len(X)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f'Fitting Reg-SVD: K={self.K}, lam={self.lam}')\n",
        "\n",
        "        # =========================================\n",
        "        # 1. Pre-compute Indices (O(N) Optimization)\n",
        "        # =========================================\n",
        "        # Avoid repeated np.where calls in the loop.\n",
        "        # Build an adjacency list for fast lookup: item -> [row_indices], user -> [row_indices]\n",
        "        item_indices_dict = defaultdict(list)\n",
        "        user_indices_dict = defaultdict(list)\n",
        "\n",
        "        for idx, (u, i) in enumerate(X):\n",
        "            user_indices_dict[u].append(idx)\n",
        "            item_indices_dict[i].append(idx)\n",
        "\n",
        "        # Convert to arrays for numpy indexing support\n",
        "        self.index_item = {k: np.array(v) for k, v in item_indices_dict.items()}\n",
        "        self.index_user = {k: np.array(v) for k, v in user_indices_dict.items()}\n",
        "\n",
        "        # =========================================\n",
        "        # 2. Initialize Biases (Static Strategy)\n",
        "        # =========================================\n",
        "        # Compute Global Bias\n",
        "        self.mu = np.mean(y)\n",
        "\n",
        "        # Compute Item Bias: b[i] = mean(y - mu) for items\n",
        "        # We calculate this once and fix it.\n",
        "        for i in range(self.n_items):\n",
        "            idx = self.index_item.get(i, [])\n",
        "            if len(idx) > 0:\n",
        "                self.b[i] = np.mean(y[idx] - self.mu)\n",
        "\n",
        "        # Compute User Bias: a[u] = mean(y - mu - b[i]) for users\n",
        "        for u in range(self.n_users):\n",
        "            idx = self.index_user.get(u, [])\n",
        "            if len(idx) > 0:\n",
        "                # Items rated by this user\n",
        "                items_rated = X[idx, 1]\n",
        "                self.a[u] = np.mean(y[idx] - self.mu - self.b[items_rated])\n",
        "\n",
        "        # =========================================\n",
        "        # 3. ALS Loop (Update P and Q only)\n",
        "        # =========================================\n",
        "        # Note: Ridge alpha needs to match the objective function scale.\n",
        "        # Obj = MSE + lam * Penalty = (1/N)*RSS + lam * Penalty\n",
        "        # Target function for Ridge is RSS + alpha * Penalty\n",
        "        # So, alpha should be lam * n_obs\n",
        "        ridge_alpha = self.lam * n_obs\n",
        "\n",
        "        for l in range(self.iterNum):\n",
        "            obj_old = self.obj(X, y)\n",
        "\n",
        "            # Update Item Latent Factors (Q)\n",
        "            for item_id in range(self.n_items):\n",
        "                idx = self.index_item.get(item_id, [])\n",
        "                if len(idx) == 0: continue\n",
        "\n",
        "                # Get data relevant to this item\n",
        "                y_subset = y[idx]         # Actual ratings\n",
        "                u_subset = X[idx, 0]      # Users who rated this item\n",
        "\n",
        "                # Target residual for matrix factorization part: y - (mu + a + b)\n",
        "                # We want P[u] * Q[i]^T approx (y - bias)\n",
        "                bias_part = self.mu + self.a[u_subset] + self.b[item_id]\n",
        "                residual_target = y_subset - bias_part\n",
        "\n",
        "                # Features: The User latent factors P for these users\n",
        "                P_features = self.P[u_subset]\n",
        "\n",
        "                # Solve for Q[item_id]\n",
        "                clf = Ridge(alpha=ridge_alpha, fit_intercept=False)\n",
        "                clf.fit(X=P_features, y=residual_target)\n",
        "                self.Q[item_id, :] = clf.coef_\n",
        "\n",
        "            # Update User Latent Factors (P)\n",
        "            for user_id in range(self.n_users):\n",
        "                idx = self.index_user.get(user_id, [])\n",
        "                if len(idx) == 0: continue\n",
        "\n",
        "                # Get data relevant to this user\n",
        "                y_subset = y[idx]\n",
        "                i_subset = X[idx, 1]      # Items rated by this user\n",
        "\n",
        "                # Target residual\n",
        "                bias_part = self.mu + self.a[user_id] + self.b[i_subset]\n",
        "                residual_target = y_subset - bias_part\n",
        "\n",
        "                # Features: The Item latent factors Q for these items\n",
        "                Q_features = self.Q[i_subset]\n",
        "\n",
        "                # Solve for P[user_id]\n",
        "                clf = Ridge(alpha=ridge_alpha, fit_intercept=False)\n",
        "                clf.fit(X=Q_features, y=residual_target)\n",
        "                self.P[user_id, :] = clf.coef_\n",
        "\n",
        "            # Check convergence\n",
        "            obj_new = self.obj(X, y)\n",
        "            diff = abs(obj_old - obj_new)\n",
        "            rmse_val = np.sqrt(self.mse(X, y))\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"RegSVD-ALS: {l+1}; obj: {obj_new:.3f}; rmse: {rmse_val:.3f}; diff: {diff:.5f}\")\n",
        "\n",
        "            if diff < self.tol:\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Vectorized prediction for user-item pairs.\n",
        "        \"\"\"\n",
        "        users = X[:, 0]\n",
        "        items = X[:, 1]\n",
        "\n",
        "        # Vectorized formula: mu + a[u] + b[i] + (P[u] . Q[i])\n",
        "        # np.sum(A * B, axis=1) computes the dot product row-wise efficiently\n",
        "        interaction = np.sum(self.P[users] * self.Q[items], axis=1)\n",
        "\n",
        "        return self.mu + self.a[users] + self.b[items] + interaction\n",
        "\n",
        "    def mse(self, X, y):\n",
        "        pred_y = self.predict(X)\n",
        "        return np.mean((pred_y - y)**2)\n",
        "\n",
        "    def obj(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes objective function: MSE + Regularization\n",
        "        \"\"\"\n",
        "        mse_tmp = self.mse(X, y)\n",
        "        # Regularization term (L2 norm of P and Q)\n",
        "        # Note: Biases are static now, so we technically only penalize P and Q updates\n",
        "        pen_tmp = np.sum(self.P**2) + np.sum(self.Q**2)\n",
        "        return mse_tmp + self.lam * pen_tmp"
      ],
      "metadata": {
        "id": "YkjsmDwLLECX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Validation: GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'lam': [0.1, 1],  # Regularization parameter\n",
        "    'K': [5, 10, 15],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    scoring='accuracy',  # You can change this to other metrics like 'f1', 'precision', 'recall'\n",
        "    n_jobs=-1,  # Use all available CPU cores\n",
        "    verbose=2  # Print progress\n",
        ")\n",
        "\n",
        "print(\"Starting grid search...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"\\nBest parameters found:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
      ],
      "metadata": {
        "id": "YatNJd55LJIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DL Keras"
      ],
      "metadata": {
        "id": "x3PEfNs7I9es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization\n",
        "from sklearn.preprocessing import StandardScaler # Value\n",
        "from sklearn.preprocessing import LabelEncoder # Label\n",
        "\n",
        "feats = None\n",
        "cate = ['cate1', 'cate2']\n",
        "dense = ['dense1', 'dense2']\n",
        "\n",
        "# StandardScaler can input numpy array or dataframe\n",
        "scaler = StandardScaler()\n",
        "feats[dense] = scaler.fit_transform(feats[dense])\n",
        "\n",
        "# Label Standardization\n",
        "for cate_tmp in cate:\n",
        "  cate_le = LabelEncoder()\n",
        "  feats[cate_tmp] = cate_le.fit_transform(feats[cate_tmp])\n",
        "\n",
        "# Data Preparation\n",
        "feats =\n"
      ],
      "metadata": {
        "id": "10QMMJDuM2jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class RSModel(keras.Model):\n",
        "  def __init__(self, n_users, embedding_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    # Embedding layers\n",
        "    self.user_embed = layers.Embedding(n_users, embedding_dim)\n",
        "\n",
        "    # Dense layers\n",
        "    self.dense = layers.Dense(hidden_dim, activation='relu')\n",
        "    ## for out layer, we don't need activation function\n",
        "\n",
        "    # Other layers\n",
        "    self.dot = layers.Dot(axes=1)\n",
        "    self.concat = layers.Concatenate()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    user_feats, item_feats = inputs\n",
        "    merged = self.concat([user_feats, item_feats])\n",
        "    sim = self.dot([user_feats, item_feats])\n",
        "    return sim\n"
      ],
      "metadata": {
        "id": "ZwHCj5y4P0yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "opt = keras.optimizers.Adam(1e-3)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['root_mean_squared_error'])\n",
        "\n",
        "## callback\n",
        "callback = [keras.callbacks.EarlyStopping(\n",
        "      monitor='val_loss',\n",
        "      patience=10,\n",
        "      mode='min',\n",
        "      restore_best_weights=True,\n",
        "  )]\n",
        "\n",
        "## fit with early stopping\n",
        "model.fit(x=X_train,\n",
        "    y=y_train,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_split=0.3,\n",
        "    callbacks=callback\n",
        "    )\n",
        "\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(X_test, y_test, batch_size=512)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "metadata": {
        "id": "9QLeAftqPzvI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}